/**
 * LLM Client
 * OpenRouterë¥¼ í†µí•œ LLM í˜¸ì¶œ + ë™ì  ëª¨ë¸ ì„ íƒ
 */

import {
  LLMContext,
  DialogueChoice,
  PersonaMood,
} from './types';
import {
  buildSystemPrompt,
  buildResponsePrompt,
  buildChoiceGenerationPrompt,
  buildEventMessagePrompt,
  EmotionalContextForPrompt,
} from './prompt-builder';
import { validateAndCorrectResponse } from './response-validator';
import {
  ModelSelector,
  ModelSelectionLogger,
  ModelConfig,
  AVAILABLE_MODELS,
} from './model-selector';
import type { TaskContext } from './model-selector';
import { getBudgetGuard } from './usage-tracker';
import type { BudgetGuard } from './usage-tracker';
import {
  parseDialogueResponse,
  parseChoicesResponse,
  parseEventMessageResponse,
  parseStoryBranchResponse,
  LLMDialogueResponseWithChoices,
} from './schemas';

const OPENROUTER_API_URL = 'https://openrouter.ai/api/v1/chat/completions';

// íƒ€ì„ìŠ¤íƒ¬í”„ í—¬í¼ í•¨ìˆ˜ (ë°€ë¦¬ì´ˆ ë‹¨ìœ„ê¹Œì§€ í‘œì‹œ)
function getTimestamp(): string {
  const now = new Date();
  return now.toISOString().replace('T', ' ').replace('Z', '');
}

interface OpenRouterMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface OpenRouterResponse {
  choices: Array<{
    message: {
      content: string;
    };
  }>;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

// ============================================
// LLM í˜¸ì¶œ ì˜µì…˜
// ============================================

export interface LLMCallOptions {
  taskContext?: TaskContext;
  forceModel?: string;
  temperature?: number;
  maxTokens?: number;
  userId?: string; // ì˜ˆì‚° ì²´í¬ìš©
  skipBudgetCheck?: boolean; // ì˜ˆì‚° ì²´í¬ ìŠ¤í‚µ (ì‹œìŠ¤í…œ í˜¸ì¶œìš©)
}

// ============================================
// LLM í´ë¼ì´ì–¸íŠ¸
// ============================================

// ============================================
// ì—ëŸ¬ í´ë˜ìŠ¤
// ============================================

export class LLMConfigError extends Error {
  constructor(message: string) {
    super(message);
    this.name = 'LLMConfigError';
  }
}

export class LLMAPIError extends Error {
  constructor(
    message: string,
    public statusCode?: number,
    public model?: string
  ) {
    super(message);
    this.name = 'LLMAPIError';
  }
}

export class LLMClient {
  private apiKey: string;
  private defaultModel: string;
  private enableDynamicSelection: boolean;
  private enableBudgetGuard: boolean;
  private budgetGuard: BudgetGuard | null = null;

  constructor(apiKey?: string, options?: {
    defaultModel?: string;
    enableDynamicSelection?: boolean;
    enableBudgetGuard?: boolean;
  }) {
    // API í‚¤ ê²€ì¦
    const resolvedApiKey = apiKey || process.env.OPENROUTER_API_KEY;

    if (!resolvedApiKey) {
      throw new LLMConfigError(
        'OPENROUTER_API_KEY is required. Set it in environment variables or pass it to constructor.'
      );
    }

    if (resolvedApiKey.length < 10) {
      throw new LLMConfigError('Invalid API key format: key is too short');
    }

    // í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ í‚¤ ì‚¬ìš© ë°©ì§€
    if (process.env.NODE_ENV === 'production' && resolvedApiKey.startsWith('sk-test-')) {
      throw new LLMConfigError('Test API key cannot be used in production environment');
    }

    this.apiKey = resolvedApiKey;
    this.defaultModel = options?.defaultModel || 'google/gemini-2.5-flash';
    this.enableDynamicSelection = options?.enableDynamicSelection ?? true;
    this.enableBudgetGuard = options?.enableBudgetGuard ?? true;
  }

  private getBudgetGuardInstance(): BudgetGuard {
    if (!this.budgetGuard) {
      this.budgetGuard = getBudgetGuard();
    }
    return this.budgetGuard;
  }

  /**
   * ëŒ€í™” ì‘ë‹µ + ì„ íƒì§€ í†µí•© ìƒì„± (ë‹¨ì¼ LLM í˜¸ì¶œ)
   * @param context - LLM ì»¨í…ìŠ¤íŠ¸ (ê¸°ì–µê³¼ ìš”ì•½ í¬í•¨ ê°€ëŠ¥)
   * @param userMessage - ìœ ì € ë©”ì‹œì§€
   * @param options - LLM í˜¸ì¶œ ì˜µì…˜
   * @returns ì‘ë‹µ + ì„ íƒì§€ê°€ í•¨ê»˜ í¬í•¨ëœ í†µí•© ê²°ê³¼
   */
  async generateResponse(
    context: LLMContext & {
      memories?: string;
      previousSummaries?: string;
      emotionalContext?: EmotionalContextForPrompt; // ê°ì • ìƒíƒœ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    },
    userMessage: string,
    options?: LLMCallOptions
  ): Promise<LLMDialogueResponseWithChoices> {
    const systemPrompt = buildSystemPrompt(context);
    // ê¸°ì–µ, ìš”ì•½, ê°ì • ì»¨í…ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    const userPrompt = buildResponsePrompt(
      context,
      userMessage,
      context.memories,
      context.previousSummaries,
      context.emotionalContext // ê°ì • ìƒíƒœ ì „ë‹¬
    );

    // ì‘ì—… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    const taskContext: TaskContext = options?.taskContext || {
      type: 'dialogue_response',
      relationshipStage: context.relationship.relationshipStage,
      affection: context.relationship.affection,
      emotionalIntensity: context.emotionalState.tensionLevel > 7 ? 'high' :
                          context.emotionalState.tensionLevel > 4 ? 'medium' : 'low',
      isVulnerableMoment: context.emotionalState.vulnerabilityShown,
      conversationLength: context.conversationHistory.length,
      requiresConsistency: true,
      requiresCreativity: true,
    };

    const response = await this.callLLM(
      [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt },
      ],
      { ...options, taskContext }
    );

    const parsedResponse = parseDialogueResponse(response.content);

    // ì‘ë‹µ ì¼ê´€ì„± ê²€ì¦ ë° ìˆ˜ì • (ê°ì • ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°)
    if (context.emotionalContext) {
      const { response: validatedResponse, wasModified, issues } =
        validateAndCorrectResponse(parsedResponse, context.emotionalContext);

      if (wasModified) {
        console.log('[LLMClient] Response was corrected for emotional consistency:', issues);
      }

      return validatedResponse;
    }

    return parsedResponse;
  }

  /**
   * ì„ íƒì§€ ìƒì„± (standard tier ì‚¬ìš©)
   */
  async generateChoices(
    context: LLMContext,
    situation: string,
    choiceCount: number = 3,
    options?: LLMCallOptions
  ): Promise<DialogueChoice[]> {
    const systemPrompt = buildSystemPrompt(context);
    const userPrompt = buildChoiceGenerationPrompt(context, situation, choiceCount);

    // ì„ íƒì§€ ìƒì„±ì€ ì¤‘ê°„ ë³µì¡ë„
    const taskContext: TaskContext = options?.taskContext || {
      type: 'choice_generation',
      relationshipStage: context.relationship.relationshipStage,
      affection: context.relationship.affection,
      requiresCreativity: true,
    };

    const response = await this.callLLM(
      [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt },
      ],
      { ...options, taskContext }
    );

    return parseChoicesResponse(response.content);
  }

  /**
   * ì´ë²¤íŠ¸ ë©”ì‹œì§€ ìƒì„± (ìƒí™©ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ)
   */
  async generateEventMessage(
    context: LLMContext,
    eventType: string,
    contextHint: string,
    options?: LLMCallOptions
  ): Promise<{ content: string; emotion: PersonaMood; postType?: string }> {
    const systemPrompt = buildSystemPrompt(context);
    const userPrompt = buildEventMessagePrompt(context, eventType, contextHint);

    // ê°ì •ì  ì´ë²¤íŠ¸ì¸ì§€ íŒë‹¨
    const isEmotional = ['comfort_user_sad_mood', 'late_night_intimate', 'react_to_premium_choice']
      .includes(contextHint);

    const taskContext: TaskContext = options?.taskContext || {
      type: 'event_message',
      relationshipStage: context.relationship.relationshipStage,
      affection: context.relationship.affection,
      emotionalIntensity: isEmotional ? 'high' : 'medium',
      isVulnerableMoment: contextHint === 'late_night_intimate',
    };

    const response = await this.callLLM(
      [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt },
      ],
      { ...options, taskContext }
    );

    return parseEventMessageResponse(response.content);
  }

  /**
   * ëŒ€í™” ìš”ì•½ ìƒì„± (economy tier - ë¹„ìš© íš¨ìœ¨)
   */
  async summarizeConversation(
    personaName: string,
    messages: Array<{ role: string; content: string }>,
    previousSummary?: string,
    options?: LLMCallOptions
  ): Promise<string> {
    const prompt = `
${previousSummary ? `Previous summary: ${previousSummary}\n\n` : ''}
Summarize the following conversation between ${personaName} and the user.
Focus on:
1. Key emotional moments
2. Important revelations or promises
3. Changes in relationship dynamic
4. Any flags or events that should be remembered

Conversation:
${messages.map(m => `[${m.role}]: ${m.content}`).join('\n')}

Provide a concise summary (max 200 words) that captures the essential context for future conversations.
`;

    // ìš”ì•½ì€ ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸ ì‚¬ìš©
    const taskContext: TaskContext = options?.taskContext || {
      type: 'conversation_summary',
      budgetConstraint: 'strict',
    };

    const response = await this.callLLM(
      [{ role: 'user', content: prompt }],
      { ...options, taskContext }
    );

    return response.content.trim();
  }

  /**
   * ìŠ¤í† ë¦¬ ë¶„ê¸°ì  ê²°ì • (premium tier - ì¤‘ìš”í•œ íŒë‹¨)
   */
  async decideStoryBranch(
    context: LLMContext,
    branchOptions: Array<{ id: string; description: string; conditions?: string }>,
    userChoice: string,
    options?: LLMCallOptions
  ): Promise<{ selectedBranch: string; reasoning: string; flagsToSet: Record<string, boolean> }> {
    const systemPrompt = buildSystemPrompt(context);
    const userPrompt = `
## STORY BRANCHING DECISION

The user made this choice: "${userChoice}"

Available story branches:
${branchOptions.map(b => `- ${b.id}: ${b.description}${b.conditions ? ` (requires: ${b.conditions})` : ''}`).join('\n')}

Current story flags: ${JSON.stringify(context.relationship.storyFlags)}
Relationship stage: ${context.relationship.relationshipStage}
Affection: ${context.relationship.affection}/100

Decide which branch fits best based on:
1. The user's choice and intent
2. Current relationship dynamics
3. Story coherence and character consistency

Respond in JSON:
{
  "selectedBranch": "branch_id",
  "reasoning": "brief explanation",
  "flagsToSet": { "flag_name": true/false }
}
`;

    // ìŠ¤í† ë¦¬ ë¶„ê¸°ëŠ” í•­ìƒ í”„ë¦¬ë¯¸ì—„ ëª¨ë¸
    const taskContext: TaskContext = options?.taskContext || {
      type: 'story_branching',
      relationshipStage: context.relationship.relationshipStage,
      affection: context.relationship.affection,
      isStoryBranching: true,
      requiresConsistency: true,
      requiresCreativity: true,
    };

    const response = await this.callLLM(
      [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt },
      ],
      { ...options, taskContext }
    );

    return parseStoryBranchResponse(response.content, branchOptions[0]?.id || 'default');
  }

  // ============================================
  // Private Methods
  // ============================================

  /**
   * LLM í˜¸ì¶œ (ë™ì  ëª¨ë¸ ì„ íƒ + ì˜ˆì‚° ì²´í¬ í¬í•¨)
   */
  private async callLLM(
    messages: OpenRouterMessage[],
    options?: LLMCallOptions
  ): Promise<{ content: string; model: string; usage?: OpenRouterResponse['usage']; budgetWarning?: string }> {
    const startTime = Date.now();
    const callId = `llm-${Date.now().toString(36)}`;

    console.log(`\n[${getTimestamp()}][${callId}] ğŸ”® LLM Call Started`);
    console.log(`[${getTimestamp()}][${callId}] Task: ${options?.taskContext?.type || 'default'}`);

    // ëª¨ë¸ ì„ íƒ
    let selectedModel: string;
    let modelConfig: ModelConfig | undefined;
    let selectionReason: string;

    if (options?.forceModel) {
      selectedModel = options.forceModel;
      modelConfig = AVAILABLE_MODELS[selectedModel];
      selectionReason = 'forced';
    } else if (this.enableDynamicSelection && options?.taskContext) {
      modelConfig = ModelSelector.selectModel(options.taskContext);
      selectedModel = modelConfig.id;
      selectionReason = 'dynamic';
    } else {
      selectedModel = this.defaultModel;
      modelConfig = AVAILABLE_MODELS[selectedModel];
      selectionReason = 'default';
    }

    console.log(`[${getTimestamp()}][${callId}] ğŸ¯ Model Selection:`);
    console.log(`  - model: ${selectedModel}`);
    console.log(`  - reason: ${selectionReason}`);
    console.log(`  - tier: ${modelConfig?.tier || 'unknown'}`);
    console.log(`  - cost: $${modelConfig?.costPer1kTokens || '?'}/1k tokens`);

    // í”„ë¡¬í”„íŠ¸ ì „ë¬¸ ë¡œê¹…
    const systemMsg = messages.find(m => m.role === 'system');
    const userMsg = messages.find(m => m.role === 'user');
    console.log(`[${getTimestamp()}][${callId}] ğŸ“ ===== FULL PROMPTS =====`);
    console.log(`[${getTimestamp()}][${callId}] ğŸ“ [SYSTEM PROMPT] (${systemMsg?.content.length || 0} chars):`);
    console.log('â”€'.repeat(60));
    console.log(systemMsg?.content || '(empty)');
    console.log('â”€'.repeat(60));
    console.log(`[${getTimestamp()}][${callId}] ğŸ“ [USER PROMPT] (${userMsg?.content.length || 0} chars):`);
    console.log('â”€'.repeat(60));
    console.log(userMsg?.content || '(empty)');
    console.log('â”€'.repeat(60));
    console.log(`[${getTimestamp()}][${callId}] ğŸ“ ===== END PROMPTS =====`)

    // ì˜ˆì‚° ì²´í¬ (ë¡œê¹…ìš© - ì°¨ë‹¨í•˜ì§€ ì•ŠìŒ)
    let budgetWarning: string | undefined;
    if (this.enableBudgetGuard && options?.userId && !options?.skipBudgetCheck) {
      const guard = this.getBudgetGuardInstance();
      const estimatedTokens = options?.maxTokens ?? (modelConfig?.maxTokens || 1000);
      const budgetCheck = await guard.preCallCheck(options.userId, selectedModel, estimatedTokens);
      budgetWarning = budgetCheck.warning;
      if (budgetWarning) {
        console.log(`[${getTimestamp()}][${callId}] âš ï¸ Budget Warning: ${budgetWarning}`);
      }
      // ì°¸ê³ : ì‹¤ì œ ì°¨ë‹¨ì€ í•˜ì§€ ì•ŠìŒ - ê°€ê²© ì •ì±…ìœ¼ë¡œ ê´€ë¦¬
    }

    console.log(`[${getTimestamp()}][${callId}] ğŸŒ Calling OpenRouter API...`);

    const response = await fetch(OPENROUTER_API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`,
        'HTTP-Referer': process.env.NEXT_PUBLIC_APP_URL || 'http://localhost:3000',
        'X-Title': 'Luminovel.ai',
      },
      body: JSON.stringify({
        model: selectedModel,
        messages,
        // ì˜¨ë„ 0.65ë¡œ ì¡°ì •: ì¼ê´€ì„± ìœ ì§€ë¥¼ ìœ„í•´ ë‚®ì¶¤ (0.8 â†’ 0.65)
        // ë„ˆë¬´ ë‚®ìœ¼ë©´ ì°½ì˜ì„± ì €í•˜, ë„ˆë¬´ ë†’ìœ¼ë©´ ìºë¦­í„° ì¼íƒˆ
        temperature: options?.temperature ?? 0.65,
        max_tokens: options?.maxTokens ?? (modelConfig?.maxTokens || 1000),
        response_format: { type: 'json_object' },
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      let errorMessage = `LLM API error (${selectedModel})`;

      try {
        const errorJson = JSON.parse(errorText);
        errorMessage = errorJson.error?.message || errorJson.message || errorText;
      } catch {
        errorMessage = errorText || `HTTP ${response.status}`;
      }

      console.error(`[${getTimestamp()}][${callId}] âŒ API Error:`, {
        model: selectedModel,
        status: response.status,
        error: errorMessage,
        taskType: options?.taskContext?.type,
        duration: `${Date.now() - startTime}ms`,
      });

      throw new LLMAPIError(errorMessage, response.status, selectedModel);
    }

    const data: OpenRouterResponse = await response.json();
    const responseTimeMs = Date.now() - startTime;

    // ì‘ë‹µ ë¡œê¹…
    const rawContent = data.choices[0]?.message?.content || '';
    console.log(`[${getTimestamp()}][${callId}] âœ… Response received (${responseTimeMs}ms)`);
    console.log(`[${getTimestamp()}][${callId}] ğŸ“Š Usage:`);
    console.log(`  - prompt_tokens: ${data.usage?.prompt_tokens || '?'}`);
    console.log(`  - completion_tokens: ${data.usage?.completion_tokens || '?'}`);
    console.log(`  - total_tokens: ${data.usage?.total_tokens || '?'}`);
    console.log(`  - estimated_cost: $${this.calculateCost(data.usage, modelConfig).toFixed(6)}`);
    console.log(`[${getTimestamp()}][${callId}] ğŸ“„ ===== FULL RESPONSE =====`);
    console.log('â”€'.repeat(60));
    console.log(rawContent);
    console.log('â”€'.repeat(60));
    console.log(`[${getTimestamp()}][${callId}] ğŸ“„ ===== END RESPONSE =====`);
    console.log(`[${getTimestamp()}][${callId}] ğŸ LLM Call completed`);

    // ë¡œê¹…
    if (options?.taskContext) {
      const complexity = this.assessComplexityForLog(options.taskContext);
      ModelSelectionLogger.log({
        taskType: options.taskContext.type,
        complexity,
        selectedModel,
        context: options.taskContext,
        responseTimeMs,
        tokenCount: data.usage?.total_tokens,
        estimatedCost: this.calculateCost(data.usage, modelConfig),
      });
    }

    // ì‚¬ìš©ëŸ‰ ê¸°ë¡ (userIdê°€ ìˆëŠ” ê²½ìš°)
    if (this.enableBudgetGuard && options?.userId && data.usage) {
      const guard = this.getBudgetGuardInstance();
      await guard.postCallRecord(
        options.userId,
        selectedModel,
        data.usage,
        options?.taskContext?.type || 'unknown'
      );
    }

    console.log(`[${callId}] ğŸ LLM Call completed\n`);

    return {
      content: data.choices[0]?.message?.content || '',
      model: selectedModel,
      usage: data.usage,
      budgetWarning,
    };
  }

  /**
   * ë³µì¡ë„ í‰ê°€ (ë¡œê¹…ìš© ê°„ì†Œí™” ë²„ì „)
   */
  private assessComplexityForLog(context: TaskContext): 'critical' | 'high' | 'medium' | 'low' {
    if (context.isStoryBranching || context.isVulnerableMoment) return 'critical';
    if (context.emotionalIntensity === 'high' || context.isPremiumContent) return 'high';
    if (context.relationshipStage === 'intimate' || context.relationshipStage === 'lover') return 'high';
    if (context.budgetConstraint === 'strict') return 'low';
    return 'medium';
  }

  /**
   * ë¹„ìš© ê³„ì‚°
   */
  private calculateCost(
    usage: OpenRouterResponse['usage'],
    modelConfig?: ModelConfig
  ): number {
    if (!usage || !modelConfig) return 0;
    return (usage.total_tokens / 1000) * modelConfig.costPer1kTokens;
  }

}

// ============================================
// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
// ============================================

let llmClientInstance: LLMClient | null = null;

export function getLLMClient(): LLMClient {
  if (!llmClientInstance) {
    llmClientInstance = new LLMClient();
  }
  return llmClientInstance;
}
